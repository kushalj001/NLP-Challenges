{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "robotics_data = pd.read_csv('transfer-learning-on-stack-exchange-tags/robotics.csv')\n",
    "diy_data = pd.read_csv('transfer-learning-on-stack-exchange-tags/diy.csv')\n",
    "biology_data = pd.read_csv('transfer-learning-on-stack-exchange-tags/biology.csv')\n",
    "crypto_data = pd.read_csv('transfer-learning-on-stack-exchange-tags/crypto.csv')\n",
    "travel_data = pd.read_csv('transfer-learning-on-stack-exchange-tags/travel.csv')\n",
    "cooking_data = pd.read_csv('transfer-learning-on-stack-exchange-tags/cooking.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([robotics_data,diy_data,biology_data,travel_data,cooking_data,crypto_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87000, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What is the right approach to write the spin c...</td>\n",
       "      <td>&lt;p&gt;Imagine programming a 3 wheel soccer robot....</td>\n",
       "      <td>soccer control</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>How can I modify a low cost hobby servo to run...</td>\n",
       "      <td>&lt;p&gt;I've got some hobby servos (&lt;a href=\"http:/...</td>\n",
       "      <td>control rcservo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>What useful gaits exist for a six legged robot...</td>\n",
       "      <td>&lt;p&gt;&lt;a href=\"http://www.oricomtech.com/projects...</td>\n",
       "      <td>gait walk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Good Microcontrollers/SOCs for a Robotics Project</td>\n",
       "      <td>&lt;p&gt;I am looking for a starting point for my pr...</td>\n",
       "      <td>microcontroller arduino raspberry-pi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Nearest-neighbor data structure for non-Euclid...</td>\n",
       "      <td>&lt;p&gt;I'm trying to implement a nearest-neighbor ...</td>\n",
       "      <td>motion-planning rrt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   1  What is the right approach to write the spin c...   \n",
       "1   2  How can I modify a low cost hobby servo to run...   \n",
       "2   3  What useful gaits exist for a six legged robot...   \n",
       "3   4  Good Microcontrollers/SOCs for a Robotics Project   \n",
       "4   5  Nearest-neighbor data structure for non-Euclid...   \n",
       "\n",
       "                                             content  \\\n",
       "0  <p>Imagine programming a 3 wheel soccer robot....   \n",
       "1  <p>I've got some hobby servos (<a href=\"http:/...   \n",
       "2  <p><a href=\"http://www.oricomtech.com/projects...   \n",
       "3  <p>I am looking for a starting point for my pr...   \n",
       "4  <p>I'm trying to implement a nearest-neighbor ...   \n",
       "\n",
       "                                   tags  \n",
       "0                        soccer control  \n",
       "1                       control rcservo  \n",
       "2                             gait walk  \n",
       "3  microcontroller arduino raspberry-pi  \n",
       "4                   motion-planning rrt  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(html):\n",
    "    soup = BeautifulSoup(html,'lxml')\n",
    "    text = soup.get_text()\n",
    "    return text\n",
    "\n",
    "data['content'] = data['content'].apply(lambda x : remove_html_tags(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "#print(stopwords)\n",
    "def remove_urls(text):\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    chars = [char for char in text if char not in string.punctuation]\n",
    "    text = ''.join([char for char in chars])\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text_lower = [x.lower() for x in text]\n",
    "    text = ''.join([x for x in text_lower])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    processed_text = [word for word in tokens if word not in stopwords]\n",
    "    processed_text = ' '.join([word for word in processed_text])\n",
    "    return processed_text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    clean_text = ' '.join([word for word in lemmas])\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['title'] = data['title'].apply(lambda x: remove_urls(x))\n",
    "data['title'] = data['title'].apply(lambda x: remove_punctuation(x))\n",
    "data['title'] = data['title'].apply(lambda x: remove_stopwords(x))\n",
    "data['title'] = data['title'].apply(lambda x: lemmatize_text(x))\n",
    "\n",
    "data['content'] = data['content'].apply(lambda x: remove_urls(x))\n",
    "data['content'] = data['content'].apply(lambda x: remove_punctuation(x))\n",
    "data['content'] = data['content'].apply(lambda x: remove_stopwords(x))\n",
    "data['content'] = data['content'].apply(lambda x: lemmatize_text(x))\n",
    "\n",
    "data['tags'] = data['tags'].apply(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>right approach write spin controller soccer robot</td>\n",
       "      <td>imagine programming 3 wheel soccer robot type ...</td>\n",
       "      <td>[soccer, control]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>modify low cost hobby servo run freely</td>\n",
       "      <td>ive got hobby servo power hd 1501mgs id like a...</td>\n",
       "      <td>[control, rcservo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>useful gait exist six legged robot pro con</td>\n",
       "      <td>tripod wave ripple improved relative pro con a...</td>\n",
       "      <td>[gait, walk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>good microcontrollerssocs robotics project</td>\n",
       "      <td>looking starting point project preferably usin...</td>\n",
       "      <td>[microcontroller, arduino, raspberry-pi]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>nearestneighbor data structure noneuclidean co...</td>\n",
       "      <td>im trying implement nearestneighbor structure ...</td>\n",
       "      <td>[motion-planning, rrt]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   1  right approach write spin controller soccer robot   \n",
       "1   2             modify low cost hobby servo run freely   \n",
       "2   3         useful gait exist six legged robot pro con   \n",
       "3   4         good microcontrollerssocs robotics project   \n",
       "4   5  nearestneighbor data structure noneuclidean co...   \n",
       "\n",
       "                                             content  \\\n",
       "0  imagine programming 3 wheel soccer robot type ...   \n",
       "1  ive got hobby servo power hd 1501mgs id like a...   \n",
       "2  tripod wave ripple improved relative pro con a...   \n",
       "3  looking starting point project preferably usin...   \n",
       "4  im trying implement nearestneighbor structure ...   \n",
       "\n",
       "                                       tags  \n",
       "0                         [soccer, control]  \n",
       "1                        [control, rcservo]  \n",
       "2                              [gait, walk]  \n",
       "3  [microcontroller, arduino, raspberry-pi]  \n",
       "4                    [motion-planning, rrt]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kushal\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "embeddings = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True,limit=600000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kushal\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('quantum_mechanics', 0.697684645652771),\n",
       " ('Physics', 0.690711259841919),\n",
       " ('quantum_physics', 0.6846468448638916),\n",
       " ('astrophysics', 0.6702420711517334),\n",
       " ('particle_physics', 0.659159779548645),\n",
       " ('thermodynamics', 0.6546549797058105),\n",
       " ('theoretical_physics', 0.6381757259368896),\n",
       " ('physicist', 0.6186479926109314),\n",
       " ('fluid_dynamics', 0.6175768375396729),\n",
       " ('quantum_theory', 0.6106815338134766)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.most_similar('physics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(87000, 4268)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_words = 50000\n",
    "max_len = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(list(data['content']))\n",
    "sequences = tokenizer.texts_to_sequences(list(data['content']))\n",
    "data_set = pad_sequences(sequences,maxlen=max_len)\n",
    "\n",
    "one_hot = preprocessing.MultiLabelBinarizer()\n",
    "one_hot_labels = one_hot.fit_transform(data['tags'])\n",
    "print(one_hot_labels.shape)                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = data_set[:int(-0.3*data_set.shape[0])]\n",
    "val_X = data_set[int(-0.3*data_set.shape[0]):]\n",
    "train_y = one_hot_labels[:int(-0.3*data_set.shape[0])]\n",
    "val_y = one_hot_labels[int(-0.3*data_set.shape[0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\kushal\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "embedding_size = 300\n",
    "for word in embeddings.wv.vocab:\n",
    "    embeddings_index[word] = embeddings.word_vec(word)\n",
    "\n",
    "all_embeddings = np.stack(list(embeddings_index.values()))\n",
    "embed_mean,embed_std = all_embeddings.mean(),all_embeddings.std()\n",
    "num_words = len(tokenizer.word_index)\n",
    "\n",
    "embedding_matrix = np.random.normal(embed_mean,embed_std,(num_words,embedding_size))\n",
    "\n",
    "for word,index in tokenizer.word_index.items():\n",
    "    index -= 1\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector\n",
    "\n",
    "del(embeddings_index)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60900 samples, validate on 26100 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47104/60900 [======================>.......] - ETA: 13:56 - loss: 7.5866e-04 - acc: 0.0000e+ - ETA: 10:57 - loss: 7.1350e-04 - acc: 0.0000e+ - ETA: 9:56 - loss: 6.8957e-04 - acc: 0.0000e+00 - ETA: 9:26 - loss: 6.8024e-04 - acc: 0.0000e+0 - ETA: 9:06 - loss: 6.7223e-04 - acc: 7.8125e-0 - ETA: 8:53 - loss: 6.7143e-04 - acc: 0.0072    - ETA: 8:47 - loss: 6.6720e-04 - acc: 0.007 - ETA: 8:45 - loss: 6.6034e-04 - acc: 0.011 - ETA: 8:37 - loss: 6.5762e-04 - acc: 0.015 - ETA: 8:31 - loss: 6.5655e-04 - acc: 0.015 - ETA: 8:24 - loss: 6.5219e-04 - acc: 0.016 - ETA: 8:19 - loss: 6.5111e-04 - acc: 0.017 - ETA: 8:13 - loss: 6.5064e-04 - acc: 0.018 - ETA: 8:09 - loss: 6.4925e-04 - acc: 0.019 - ETA: 8:05 - loss: 6.4602e-04 - acc: 0.019 - ETA: 8:02 - loss: 6.4447e-04 - acc: 0.019 - ETA: 7:58 - loss: 6.4299e-04 - acc: 0.018 - ETA: 7:55 - loss: 6.4269e-04 - acc: 0.019 - ETA: 7:51 - loss: 6.4221e-04 - acc: 0.018 - ETA: 7:48 - loss: 6.4235e-04 - acc: 0.018 - ETA: 7:46 - loss: 6.4019e-04 - acc: 0.017 - ETA: 7:44 - loss: 6.3891e-04 - acc: 0.017 - ETA: 7:44 - loss: 6.3719e-04 - acc: 0.017 - ETA: 7:43 - loss: 6.3725e-04 - acc: 0.016 - ETA: 7:42 - loss: 6.3639e-04 - acc: 0.016 - ETA: 7:39 - loss: 6.3647e-04 - acc: 0.016 - ETA: 7:36 - loss: 6.3597e-04 - acc: 0.015 - ETA: 7:33 - loss: 6.3598e-04 - acc: 0.015 - ETA: 7:30 - loss: 6.3537e-04 - acc: 0.015 - ETA: 7:28 - loss: 6.3582e-04 - acc: 0.015 - ETA: 7:26 - loss: 6.3480e-04 - acc: 0.015 - ETA: 7:24 - loss: 6.3514e-04 - acc: 0.015 - ETA: 7:24 - loss: 6.3366e-04 - acc: 0.014 - ETA: 7:25 - loss: 6.3331e-04 - acc: 0.015 - ETA: 7:22 - loss: 6.3269e-04 - acc: 0.016 - ETA: 7:22 - loss: 6.3263e-04 - acc: 0.016 - ETA: 7:22 - loss: 6.3259e-04 - acc: 0.016 - ETA: 7:21 - loss: 6.3176e-04 - acc: 0.016 - ETA: 7:22 - loss: 6.3171e-04 - acc: 0.017 - ETA: 7:24 - loss: 6.3233e-04 - acc: 0.017 - ETA: 7:26 - loss: 6.3216e-04 - acc: 0.018 - ETA: 7:29 - loss: 6.3222e-04 - acc: 0.018 - ETA: 7:32 - loss: 6.3143e-04 - acc: 0.019 - ETA: 7:34 - loss: 6.3143e-04 - acc: 0.019 - ETA: 7:35 - loss: 6.3147e-04 - acc: 0.020 - ETA: 7:37 - loss: 6.3144e-04 - acc: 0.020 - ETA: 7:39 - loss: 6.3115e-04 - acc: 0.020 - ETA: 7:39 - loss: 6.3047e-04 - acc: 0.021 - ETA: 7:39 - loss: 6.3043e-04 - acc: 0.021 - ETA: 7:39 - loss: 6.3011e-04 - acc: 0.022 - ETA: 7:37 - loss: 6.3063e-04 - acc: 0.023 - ETA: 7:36 - loss: 6.3038e-04 - acc: 0.023 - ETA: 7:36 - loss: 6.3007e-04 - acc: 0.023 - ETA: 7:36 - loss: 6.2939e-04 - acc: 0.024 - ETA: 7:34 - loss: 6.2927e-04 - acc: 0.024 - ETA: 7:33 - loss: 6.2919e-04 - acc: 0.024 - ETA: 7:32 - loss: 6.2927e-04 - acc: 0.024 - ETA: 7:30 - loss: 6.2919e-04 - acc: 0.025 - ETA: 7:29 - loss: 6.2949e-04 - acc: 0.025 - ETA: 7:28 - loss: 6.2910e-04 - acc: 0.026 - ETA: 7:27 - loss: 6.2894e-04 - acc: 0.026 - ETA: 7:25 - loss: 6.2858e-04 - acc: 0.027 - ETA: 7:24 - loss: 6.2843e-04 - acc: 0.027 - ETA: 7:23 - loss: 6.2794e-04 - acc: 0.027 - ETA: 7:22 - loss: 6.2765e-04 - acc: 0.028 - ETA: 7:20 - loss: 6.2733e-04 - acc: 0.028 - ETA: 7:18 - loss: 6.2708e-04 - acc: 0.028 - ETA: 7:17 - loss: 6.2702e-04 - acc: 0.029 - ETA: 7:16 - loss: 6.2657e-04 - acc: 0.029 - ETA: 7:14 - loss: 6.2684e-04 - acc: 0.029 - ETA: 7:13 - loss: 6.2633e-04 - acc: 0.029 - ETA: 7:11 - loss: 6.2622e-04 - acc: 0.029 - ETA: 7:09 - loss: 6.2666e-04 - acc: 0.030 - ETA: 7:07 - loss: 6.2673e-04 - acc: 0.030 - ETA: 7:05 - loss: 6.2667e-04 - acc: 0.031 - ETA: 7:03 - loss: 6.2676e-04 - acc: 0.031 - ETA: 7:01 - loss: 6.2670e-04 - acc: 0.031 - ETA: 6:59 - loss: 6.2646e-04 - acc: 0.031 - ETA: 6:57 - loss: 6.2642e-04 - acc: 0.031 - ETA: 6:55 - loss: 6.2606e-04 - acc: 0.031 - ETA: 6:53 - loss: 6.2584e-04 - acc: 0.032 - ETA: 6:50 - loss: 6.2615e-04 - acc: 0.032 - ETA: 6:48 - loss: 6.2625e-04 - acc: 0.032 - ETA: 6:46 - loss: 6.2654e-04 - acc: 0.032 - ETA: 6:43 - loss: 6.2670e-04 - acc: 0.032 - ETA: 6:41 - loss: 6.2652e-04 - acc: 0.033 - ETA: 6:39 - loss: 6.2631e-04 - acc: 0.033 - ETA: 6:37 - loss: 6.2641e-04 - acc: 0.033 - ETA: 6:35 - loss: 6.2621e-04 - acc: 0.033 - ETA: 6:32 - loss: 6.2622e-04 - acc: 0.034 - ETA: 6:30 - loss: 6.2603e-04 - acc: 0.034 - ETA: 6:27 - loss: 6.2568e-04 - acc: 0.034 - ETA: 6:25 - loss: 6.2564e-04 - acc: 0.034 - ETA: 6:23 - loss: 6.2538e-04 - acc: 0.035 - ETA: 6:21 - loss: 6.2525e-04 - acc: 0.035 - ETA: 6:17 - loss: 6.2517e-04 - acc: 0.035 - ETA: 6:14 - loss: 6.2510e-04 - acc: 0.035 - ETA: 6:12 - loss: 6.2542e-04 - acc: 0.035 - ETA: 6:10 - loss: 6.2570e-04 - acc: 0.035 - ETA: 6:08 - loss: 6.2579e-04 - acc: 0.036 - ETA: 6:05 - loss: 6.2613e-04 - acc: 0.036 - ETA: 6:02 - loss: 6.2549e-04 - acc: 0.036 - ETA: 6:00 - loss: 6.2544e-04 - acc: 0.036 - ETA: 5:57 - loss: 6.2516e-04 - acc: 0.036 - ETA: 5:54 - loss: 6.2533e-04 - acc: 0.036 - ETA: 5:51 - loss: 6.2539e-04 - acc: 0.036 - ETA: 5:48 - loss: 6.2539e-04 - acc: 0.037 - ETA: 5:45 - loss: 6.2541e-04 - acc: 0.037 - ETA: 5:42 - loss: 6.2538e-04 - acc: 0.037 - ETA: 5:39 - loss: 6.2546e-04 - acc: 0.037 - ETA: 5:37 - loss: 6.2580e-04 - acc: 0.037 - ETA: 5:34 - loss: 6.2562e-04 - acc: 0.037 - ETA: 5:31 - loss: 6.2556e-04 - acc: 0.037 - ETA: 5:28 - loss: 6.2547e-04 - acc: 0.037 - ETA: 5:26 - loss: 6.2500e-04 - acc: 0.037 - ETA: 5:23 - loss: 6.2478e-04 - acc: 0.038 - ETA: 5:21 - loss: 6.2477e-04 - acc: 0.038 - ETA: 5:18 - loss: 6.2478e-04 - acc: 0.038 - ETA: 5:15 - loss: 6.2448e-04 - acc: 0.038 - ETA: 5:13 - loss: 6.2443e-04 - acc: 0.038 - ETA: 5:10 - loss: 6.2419e-04 - acc: 0.038 - ETA: 5:07 - loss: 6.2411e-04 - acc: 0.038 - ETA: 5:04 - loss: 6.2398e-04 - acc: 0.039 - ETA: 5:01 - loss: 6.2407e-04 - acc: 0.039 - ETA: 4:58 - loss: 6.2428e-04 - acc: 0.039 - ETA: 4:56 - loss: 6.2435e-04 - acc: 0.039 - ETA: 4:54 - loss: 6.2462e-04 - acc: 0.039 - ETA: 4:52 - loss: 6.2452e-04 - acc: 0.039 - ETA: 4:49 - loss: 6.2455e-04 - acc: 0.040 - ETA: 4:47 - loss: 6.2463e-04 - acc: 0.040 - ETA: 4:44 - loss: 6.2467e-04 - acc: 0.040 - ETA: 4:42 - loss: 6.2473e-04 - acc: 0.040 - ETA: 4:39 - loss: 6.2471e-04 - acc: 0.040 - ETA: 4:37 - loss: 6.2474e-04 - acc: 0.040 - ETA: 4:34 - loss: 6.2454e-04 - acc: 0.040 - ETA: 4:32 - loss: 6.2479e-04 - acc: 0.040 - ETA: 4:29 - loss: 6.2482e-04 - acc: 0.041 - ETA: 4:27 - loss: 6.2479e-04 - acc: 0.041 - ETA: 4:24 - loss: 6.2473e-04 - acc: 0.041 - ETA: 4:22 - loss: 6.2466e-04 - acc: 0.041 - ETA: 4:20 - loss: 6.2458e-04 - acc: 0.041 - ETA: 4:17 - loss: 6.2461e-04 - acc: 0.041 - ETA: 4:15 - loss: 6.2471e-04 - acc: 0.041 - ETA: 4:13 - loss: 6.2456e-04 - acc: 0.041 - ETA: 4:10 - loss: 6.2458e-04 - acc: 0.041 - ETA: 4:07 - loss: 6.2460e-04 - acc: 0.041 - ETA: 4:05 - loss: 6.2471e-04 - acc: 0.041 - ETA: 4:02 - loss: 6.2490e-04 - acc: 0.041 - ETA: 4:00 - loss: 6.2458e-04 - acc: 0.041 - ETA: 3:57 - loss: 6.2465e-04 - acc: 0.041 - ETA: 3:55 - loss: 6.2449e-04 - acc: 0.041 - ETA: 3:52 - loss: 6.2458e-04 - acc: 0.042 - ETA: 3:49 - loss: 6.2458e-04 - acc: 0.042 - ETA: 3:47 - loss: 6.2437e-04 - acc: 0.042 - ETA: 3:44 - loss: 6.2440e-04 - acc: 0.041 - ETA: 3:42 - loss: 6.2437e-04 - acc: 0.041 - ETA: 3:39 - loss: 6.2450e-04 - acc: 0.041 - ETA: 3:36 - loss: 6.2469e-04 - acc: 0.042 - ETA: 3:34 - loss: 6.2457e-04 - acc: 0.041 - ETA: 3:31 - loss: 6.2466e-04 - acc: 0.042 - ETA: 3:29 - loss: 6.2468e-04 - acc: 0.042 - ETA: 3:26 - loss: 6.2455e-04 - acc: 0.042 - ETA: 3:23 - loss: 6.2439e-04 - acc: 0.042 - ETA: 3:21 - loss: 6.2448e-04 - acc: 0.042 - ETA: 3:18 - loss: 6.2466e-04 - acc: 0.042 - ETA: 3:16 - loss: 6.2461e-04 - acc: 0.042 - ETA: 3:13 - loss: 6.2473e-04 - acc: 0.042 - ETA: 3:10 - loss: 6.2446e-04 - acc: 0.042 - ETA: 3:08 - loss: 6.2452e-04 - acc: 0.042 - ETA: 3:05 - loss: 6.2449e-04 - acc: 0.042 - ETA: 3:03 - loss: 6.2454e-04 - acc: 0.042 - ETA: 3:00 - loss: 6.2452e-04 - acc: 0.042 - ETA: 2:57 - loss: 6.2458e-04 - acc: 0.042 - ETA: 2:55 - loss: 6.2461e-04 - acc: 0.042 - ETA: 2:52 - loss: 6.2455e-04 - acc: 0.042 - ETA: 2:49 - loss: 6.2459e-04 - acc: 0.042 - ETA: 2:47 - loss: 6.2451e-04 - acc: 0.043 - ETA: 2:44 - loss: 6.2441e-04 - acc: 0.043 - ETA: 2:41 - loss: 6.2418e-04 - acc: 0.043 - ETA: 2:39 - loss: 6.2419e-04 - acc: 0.043 - ETA: 2:36 - loss: 6.2423e-04 - acc: 0.043 - ETA: 2:33 - loss: 6.2427e-04 - acc: 0.043 - ETA: 2:31 - loss: 6.2447e-04 - acc: 0.043 - ETA: 2:28 - loss: 6.2449e-04 - acc: 0.04360900/60900 [==============================] - ETA: 2:25 - loss: 6.2443e-04 - acc: 0.043 - ETA: 2:22 - loss: 6.2465e-04 - acc: 0.043 - ETA: 2:19 - loss: 6.2442e-04 - acc: 0.043 - ETA: 2:17 - loss: 6.2449e-04 - acc: 0.043 - ETA: 2:14 - loss: 6.2457e-04 - acc: 0.043 - ETA: 2:12 - loss: 6.2444e-04 - acc: 0.043 - ETA: 2:09 - loss: 6.2445e-04 - acc: 0.043 - ETA: 2:06 - loss: 6.2449e-04 - acc: 0.043 - ETA: 2:03 - loss: 6.2435e-04 - acc: 0.043 - ETA: 2:01 - loss: 6.2431e-04 - acc: 0.043 - ETA: 1:58 - loss: 6.2444e-04 - acc: 0.043 - ETA: 1:55 - loss: 6.2450e-04 - acc: 0.043 - ETA: 1:53 - loss: 6.2453e-04 - acc: 0.043 - ETA: 1:50 - loss: 6.2455e-04 - acc: 0.043 - ETA: 1:47 - loss: 6.2460e-04 - acc: 0.043 - ETA: 1:44 - loss: 6.2460e-04 - acc: 0.043 - ETA: 1:42 - loss: 6.2446e-04 - acc: 0.043 - ETA: 1:39 - loss: 6.2451e-04 - acc: 0.043 - ETA: 1:36 - loss: 6.2438e-04 - acc: 0.043 - ETA: 1:33 - loss: 6.2424e-04 - acc: 0.043 - ETA: 1:31 - loss: 6.2424e-04 - acc: 0.044 - ETA: 1:28 - loss: 6.2434e-04 - acc: 0.044 - ETA: 1:25 - loss: 6.2415e-04 - acc: 0.043 - ETA: 1:22 - loss: 6.2416e-04 - acc: 0.043 - ETA: 1:20 - loss: 6.2424e-04 - acc: 0.043 - ETA: 1:17 - loss: 6.2420e-04 - acc: 0.044 - ETA: 1:14 - loss: 6.2423e-04 - acc: 0.044 - ETA: 1:11 - loss: 6.2411e-04 - acc: 0.044 - ETA: 1:09 - loss: 6.2406e-04 - acc: 0.044 - ETA: 1:06 - loss: 6.2403e-04 - acc: 0.044 - ETA: 1:03 - loss: 6.2401e-04 - acc: 0.044 - ETA: 1:00 - loss: 6.2399e-04 - acc: 0.044 - ETA: 58s - loss: 6.2398e-04 - acc: 0.044 - ETA: 55s - loss: 6.2397e-04 - acc: 0.04 - ETA: 52s - loss: 6.2397e-04 - acc: 0.04 - ETA: 49s - loss: 6.2404e-04 - acc: 0.04 - ETA: 46s - loss: 6.2409e-04 - acc: 0.04 - ETA: 44s - loss: 6.2420e-04 - acc: 0.04 - ETA: 41s - loss: 6.2422e-04 - acc: 0.04 - ETA: 38s - loss: 6.2415e-04 - acc: 0.04 - ETA: 35s - loss: 6.2403e-04 - acc: 0.04 - ETA: 33s - loss: 6.2404e-04 - acc: 0.04 - ETA: 30s - loss: 6.2402e-04 - acc: 0.04 - ETA: 27s - loss: 6.2400e-04 - acc: 0.04 - ETA: 24s - loss: 6.2391e-04 - acc: 0.04 - ETA: 22s - loss: 6.2400e-04 - acc: 0.04 - ETA: 19s - loss: 6.2411e-04 - acc: 0.04 - ETA: 16s - loss: 6.2407e-04 - acc: 0.04 - ETA: 13s - loss: 6.2409e-04 - acc: 0.04 - ETA: 10s - loss: 6.2413e-04 - acc: 0.04 - ETA: 8s - loss: 6.2403e-04 - acc: 0.0449 - ETA: 5s - loss: 6.2419e-04 - acc: 0.044 - ETA: 2s - loss: 6.2408e-04 - acc: 0.044 - 735s 12ms/step - loss: 6.2397e-04 - acc: 0.0449 - val_loss: 5.6127e-04 - val_acc: 0.0000e+00\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47616/60900 [======================>.......] - ETA: 12:07 - loss: 6.2708e-04 - acc: 0.05 - ETA: 11:50 - loss: 6.3271e-04 - acc: 0.06 - ETA: 11:56 - loss: 6.3004e-04 - acc: 0.05 - ETA: 12:06 - loss: 6.2740e-04 - acc: 0.05 - ETA: 12:12 - loss: 6.1994e-04 - acc: 0.05 - ETA: 12:16 - loss: 6.1940e-04 - acc: 0.05 - ETA: 12:21 - loss: 6.2068e-04 - acc: 0.05 - ETA: 12:28 - loss: 6.2126e-04 - acc: 0.05 - ETA: 12:28 - loss: 6.2283e-04 - acc: 0.05 - ETA: 12:24 - loss: 6.2311e-04 - acc: 0.05 - ETA: 12:19 - loss: 6.2271e-04 - acc: 0.05 - ETA: 12:11 - loss: 6.2383e-04 - acc: 0.05 - ETA: 12:00 - loss: 6.2398e-04 - acc: 0.05 - ETA: 11:54 - loss: 6.2539e-04 - acc: 0.05 - ETA: 11:50 - loss: 6.2602e-04 - acc: 0.05 - ETA: 11:45 - loss: 6.2525e-04 - acc: 0.05 - ETA: 11:37 - loss: 6.2543e-04 - acc: 0.05 - ETA: 11:32 - loss: 6.2384e-04 - acc: 0.05 - ETA: 11:29 - loss: 6.2331e-04 - acc: 0.05 - ETA: 11:25 - loss: 6.2366e-04 - acc: 0.05 - ETA: 11:22 - loss: 6.2327e-04 - acc: 0.05 - ETA: 11:18 - loss: 6.2439e-04 - acc: 0.05 - ETA: 11:14 - loss: 6.2320e-04 - acc: 0.05 - ETA: 11:11 - loss: 6.2253e-04 - acc: 0.05 - ETA: 11:07 - loss: 6.2287e-04 - acc: 0.05 - ETA: 11:01 - loss: 6.2278e-04 - acc: 0.05 - ETA: 10:57 - loss: 6.2390e-04 - acc: 0.05 - ETA: 10:53 - loss: 6.2322e-04 - acc: 0.05 - ETA: 10:50 - loss: 6.2384e-04 - acc: 0.05 - ETA: 10:47 - loss: 6.2328e-04 - acc: 0.05 - ETA: 10:43 - loss: 6.2374e-04 - acc: 0.05 - ETA: 10:40 - loss: 6.2435e-04 - acc: 0.05 - ETA: 10:36 - loss: 6.2427e-04 - acc: 0.05 - ETA: 10:31 - loss: 6.2360e-04 - acc: 0.05 - ETA: 10:27 - loss: 6.2327e-04 - acc: 0.05 - ETA: 10:24 - loss: 6.2341e-04 - acc: 0.05 - ETA: 10:21 - loss: 6.2318e-04 - acc: 0.05 - ETA: 10:16 - loss: 6.2365e-04 - acc: 0.05 - ETA: 10:12 - loss: 6.2367e-04 - acc: 0.05 - ETA: 10:09 - loss: 6.2370e-04 - acc: 0.05 - ETA: 10:06 - loss: 6.2410e-04 - acc: 0.05 - ETA: 10:01 - loss: 6.2395e-04 - acc: 0.05 - ETA: 9:58 - loss: 6.2387e-04 - acc: 0.0506 - ETA: 9:55 - loss: 6.2366e-04 - acc: 0.050 - ETA: 9:52 - loss: 6.2399e-04 - acc: 0.051 - ETA: 9:50 - loss: 6.2382e-04 - acc: 0.051 - ETA: 9:47 - loss: 6.2343e-04 - acc: 0.051 - ETA: 9:44 - loss: 6.2321e-04 - acc: 0.051 - ETA: 9:41 - loss: 6.2311e-04 - acc: 0.051 - ETA: 9:38 - loss: 6.2279e-04 - acc: 0.051 - ETA: 9:34 - loss: 6.2294e-04 - acc: 0.051 - ETA: 9:31 - loss: 6.2318e-04 - acc: 0.051 - ETA: 9:28 - loss: 6.2383e-04 - acc: 0.051 - ETA: 9:25 - loss: 6.2378e-04 - acc: 0.051 - ETA: 9:22 - loss: 6.2365e-04 - acc: 0.051 - ETA: 9:19 - loss: 6.2399e-04 - acc: 0.051 - ETA: 9:15 - loss: 6.2406e-04 - acc: 0.051 - ETA: 9:12 - loss: 6.2392e-04 - acc: 0.051 - ETA: 9:08 - loss: 6.2417e-04 - acc: 0.051 - ETA: 9:05 - loss: 6.2362e-04 - acc: 0.051 - ETA: 9:02 - loss: 6.2322e-04 - acc: 0.051 - ETA: 8:59 - loss: 6.2384e-04 - acc: 0.050 - ETA: 8:55 - loss: 6.2359e-04 - acc: 0.050 - ETA: 8:51 - loss: 6.2360e-04 - acc: 0.050 - ETA: 8:49 - loss: 6.2337e-04 - acc: 0.050 - ETA: 8:45 - loss: 6.2336e-04 - acc: 0.050 - ETA: 8:42 - loss: 6.2351e-04 - acc: 0.050 - ETA: 8:38 - loss: 6.2367e-04 - acc: 0.049 - ETA: 8:35 - loss: 6.2344e-04 - acc: 0.049 - ETA: 8:32 - loss: 6.2345e-04 - acc: 0.049 - ETA: 8:28 - loss: 6.2371e-04 - acc: 0.049 - ETA: 8:25 - loss: 6.2321e-04 - acc: 0.049 - ETA: 8:22 - loss: 6.2333e-04 - acc: 0.049 - ETA: 8:19 - loss: 6.2354e-04 - acc: 0.049 - ETA: 8:15 - loss: 6.2340e-04 - acc: 0.049 - ETA: 8:12 - loss: 6.2325e-04 - acc: 0.049 - ETA: 8:09 - loss: 6.2284e-04 - acc: 0.049 - ETA: 8:06 - loss: 6.2310e-04 - acc: 0.049 - ETA: 8:03 - loss: 6.2313e-04 - acc: 0.049 - ETA: 7:59 - loss: 6.2315e-04 - acc: 0.049 - ETA: 7:56 - loss: 6.2351e-04 - acc: 0.049 - ETA: 7:53 - loss: 6.2336e-04 - acc: 0.049 - ETA: 7:50 - loss: 6.2367e-04 - acc: 0.048 - ETA: 7:47 - loss: 6.2360e-04 - acc: 0.048 - ETA: 7:45 - loss: 6.2364e-04 - acc: 0.048 - ETA: 7:42 - loss: 6.2371e-04 - acc: 0.048 - ETA: 7:38 - loss: 6.2353e-04 - acc: 0.048 - ETA: 7:35 - loss: 6.2343e-04 - acc: 0.048 - ETA: 7:32 - loss: 6.2374e-04 - acc: 0.047 - ETA: 7:28 - loss: 6.2392e-04 - acc: 0.047 - ETA: 7:25 - loss: 6.2391e-04 - acc: 0.047 - ETA: 7:23 - loss: 6.2421e-04 - acc: 0.047 - ETA: 7:20 - loss: 6.2384e-04 - acc: 0.047 - ETA: 7:17 - loss: 6.2365e-04 - acc: 0.047 - ETA: 7:13 - loss: 6.2352e-04 - acc: 0.047 - ETA: 7:10 - loss: 6.2376e-04 - acc: 0.047 - ETA: 7:07 - loss: 6.2402e-04 - acc: 0.046 - ETA: 7:04 - loss: 6.2332e-04 - acc: 0.046 - ETA: 7:01 - loss: 6.2350e-04 - acc: 0.046 - ETA: 6:58 - loss: 6.2347e-04 - acc: 0.046 - ETA: 6:55 - loss: 6.2339e-04 - acc: 0.046 - ETA: 6:52 - loss: 6.2336e-04 - acc: 0.046 - ETA: 6:49 - loss: 6.2322e-04 - acc: 0.046 - ETA: 6:46 - loss: 6.2305e-04 - acc: 0.045 - ETA: 6:43 - loss: 6.2303e-04 - acc: 0.045 - ETA: 6:43 - loss: 6.2307e-04 - acc: 0.045 - ETA: 6:40 - loss: 6.2323e-04 - acc: 0.045 - ETA: 6:38 - loss: 6.2311e-04 - acc: 0.045 - ETA: 6:36 - loss: 6.2289e-04 - acc: 0.045 - ETA: 6:33 - loss: 6.2308e-04 - acc: 0.044 - ETA: 6:33 - loss: 6.2306e-04 - acc: 0.044 - ETA: 6:33 - loss: 6.2279e-04 - acc: 0.045 - ETA: 6:29 - loss: 6.2293e-04 - acc: 0.044 - ETA: 6:27 - loss: 6.2310e-04 - acc: 0.044 - ETA: 6:24 - loss: 6.2335e-04 - acc: 0.044 - ETA: 6:21 - loss: 6.2339e-04 - acc: 0.044 - ETA: 6:17 - loss: 6.2349e-04 - acc: 0.044 - ETA: 6:14 - loss: 6.2356e-04 - acc: 0.044 - ETA: 6:11 - loss: 6.2355e-04 - acc: 0.044 - ETA: 6:08 - loss: 6.2350e-04 - acc: 0.044 - ETA: 6:05 - loss: 6.2337e-04 - acc: 0.043 - ETA: 6:02 - loss: 6.2340e-04 - acc: 0.043 - ETA: 5:59 - loss: 6.2342e-04 - acc: 0.043 - ETA: 5:56 - loss: 6.2358e-04 - acc: 0.043 - ETA: 5:53 - loss: 6.2352e-04 - acc: 0.043 - ETA: 5:50 - loss: 6.2345e-04 - acc: 0.043 - ETA: 5:47 - loss: 6.2324e-04 - acc: 0.043 - ETA: 5:44 - loss: 6.2310e-04 - acc: 0.043 - ETA: 5:41 - loss: 6.2301e-04 - acc: 0.043 - ETA: 5:37 - loss: 6.2282e-04 - acc: 0.043 - ETA: 5:34 - loss: 6.2288e-04 - acc: 0.043 - ETA: 5:31 - loss: 6.2280e-04 - acc: 0.043 - ETA: 5:29 - loss: 6.2273e-04 - acc: 0.043 - ETA: 5:26 - loss: 6.2268e-04 - acc: 0.043 - ETA: 5:23 - loss: 6.2248e-04 - acc: 0.043 - ETA: 5:20 - loss: 6.2240e-04 - acc: 0.043 - ETA: 5:17 - loss: 6.2256e-04 - acc: 0.043 - ETA: 5:14 - loss: 6.2274e-04 - acc: 0.043 - ETA: 5:11 - loss: 6.2268e-04 - acc: 0.043 - ETA: 5:08 - loss: 6.2277e-04 - acc: 0.043 - ETA: 5:05 - loss: 6.2274e-04 - acc: 0.043 - ETA: 5:01 - loss: 6.2301e-04 - acc: 0.043 - ETA: 4:58 - loss: 6.2305e-04 - acc: 0.043 - ETA: 4:55 - loss: 6.2326e-04 - acc: 0.043 - ETA: 4:52 - loss: 6.2319e-04 - acc: 0.043 - ETA: 4:50 - loss: 6.2310e-04 - acc: 0.043 - ETA: 4:46 - loss: 6.2308e-04 - acc: 0.043 - ETA: 4:43 - loss: 6.2307e-04 - acc: 0.043 - ETA: 4:40 - loss: 6.2294e-04 - acc: 0.043 - ETA: 4:37 - loss: 6.2284e-04 - acc: 0.043 - ETA: 4:34 - loss: 6.2274e-04 - acc: 0.043 - ETA: 4:31 - loss: 6.2264e-04 - acc: 0.043 - ETA: 4:28 - loss: 6.2274e-04 - acc: 0.043 - ETA: 4:25 - loss: 6.2272e-04 - acc: 0.043 - ETA: 4:22 - loss: 6.2265e-04 - acc: 0.043 - ETA: 4:18 - loss: 6.2259e-04 - acc: 0.043 - ETA: 4:15 - loss: 6.2249e-04 - acc: 0.043 - ETA: 4:12 - loss: 6.2243e-04 - acc: 0.043 - ETA: 4:08 - loss: 6.2247e-04 - acc: 0.043 - ETA: 4:05 - loss: 6.2253e-04 - acc: 0.043 - ETA: 4:02 - loss: 6.2246e-04 - acc: 0.043 - ETA: 3:59 - loss: 6.2253e-04 - acc: 0.043 - ETA: 3:56 - loss: 6.2260e-04 - acc: 0.043 - ETA: 3:53 - loss: 6.2256e-04 - acc: 0.043 - ETA: 3:49 - loss: 6.2256e-04 - acc: 0.043 - ETA: 3:46 - loss: 6.2241e-04 - acc: 0.043 - ETA: 3:43 - loss: 6.2245e-04 - acc: 0.043 - ETA: 3:40 - loss: 6.2255e-04 - acc: 0.043 - ETA: 3:37 - loss: 6.2262e-04 - acc: 0.043 - ETA: 3:33 - loss: 6.2245e-04 - acc: 0.043 - ETA: 3:30 - loss: 6.2247e-04 - acc: 0.043 - ETA: 3:27 - loss: 6.2241e-04 - acc: 0.043 - ETA: 3:24 - loss: 6.2229e-04 - acc: 0.043 - ETA: 3:21 - loss: 6.2220e-04 - acc: 0.043 - ETA: 3:18 - loss: 6.2232e-04 - acc: 0.043 - ETA: 3:15 - loss: 6.2211e-04 - acc: 0.043 - ETA: 3:12 - loss: 6.2196e-04 - acc: 0.043 - ETA: 3:09 - loss: 6.2197e-04 - acc: 0.043 - ETA: 3:05 - loss: 6.2199e-04 - acc: 0.043 - ETA: 3:02 - loss: 6.2193e-04 - acc: 0.043 - ETA: 2:59 - loss: 6.2168e-04 - acc: 0.043 - ETA: 2:56 - loss: 6.2177e-04 - acc: 0.043 - ETA: 2:53 - loss: 6.2174e-04 - acc: 0.043 - ETA: 2:49 - loss: 6.2188e-04 - acc: 0.043 - ETA: 2:46 - loss: 6.2185e-04 - acc: 0.043 - ETA: 2:43 - loss: 6.2192e-04 - acc: 0.0434"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60900/60900 [==============================] - ETA: 2:40 - loss: 6.2191e-04 - acc: 0.043 - ETA: 2:37 - loss: 6.2169e-04 - acc: 0.043 - ETA: 2:34 - loss: 6.2174e-04 - acc: 0.043 - ETA: 2:30 - loss: 6.2169e-04 - acc: 0.043 - ETA: 2:27 - loss: 6.2165e-04 - acc: 0.043 - ETA: 2:24 - loss: 6.2164e-04 - acc: 0.043 - ETA: 2:21 - loss: 6.2163e-04 - acc: 0.043 - ETA: 2:18 - loss: 6.2163e-04 - acc: 0.043 - ETA: 2:15 - loss: 6.2155e-04 - acc: 0.043 - ETA: 2:11 - loss: 6.2164e-04 - acc: 0.043 - ETA: 2:08 - loss: 6.2154e-04 - acc: 0.043 - ETA: 2:05 - loss: 6.2135e-04 - acc: 0.043 - ETA: 2:02 - loss: 6.2123e-04 - acc: 0.043 - ETA: 1:59 - loss: 6.2128e-04 - acc: 0.043 - ETA: 1:56 - loss: 6.2131e-04 - acc: 0.043 - ETA: 1:52 - loss: 6.2133e-04 - acc: 0.043 - ETA: 1:49 - loss: 6.2132e-04 - acc: 0.043 - ETA: 1:46 - loss: 6.2131e-04 - acc: 0.043 - ETA: 1:43 - loss: 6.2130e-04 - acc: 0.043 - ETA: 1:40 - loss: 6.2125e-04 - acc: 0.043 - ETA: 1:36 - loss: 6.2133e-04 - acc: 0.043 - ETA: 1:33 - loss: 6.2139e-04 - acc: 0.042 - ETA: 1:30 - loss: 6.2145e-04 - acc: 0.043 - ETA: 1:27 - loss: 6.2147e-04 - acc: 0.042 - ETA: 1:24 - loss: 6.2143e-04 - acc: 0.042 - ETA: 1:21 - loss: 6.2143e-04 - acc: 0.042 - ETA: 1:18 - loss: 6.2148e-04 - acc: 0.042 - ETA: 1:14 - loss: 6.2141e-04 - acc: 0.043 - ETA: 1:11 - loss: 6.2139e-04 - acc: 0.043 - ETA: 1:08 - loss: 6.2133e-04 - acc: 0.043 - ETA: 1:05 - loss: 6.2129e-04 - acc: 0.043 - ETA: 1:02 - loss: 6.2130e-04 - acc: 0.043 - ETA: 59s - loss: 6.2136e-04 - acc: 0.043 - ETA: 56s - loss: 6.2127e-04 - acc: 0.04 - ETA: 52s - loss: 6.2113e-04 - acc: 0.04 - ETA: 49s - loss: 6.2114e-04 - acc: 0.04 - ETA: 46s - loss: 6.2109e-04 - acc: 0.04 - ETA: 43s - loss: 6.2110e-04 - acc: 0.04 - ETA: 40s - loss: 6.2108e-04 - acc: 0.04 - ETA: 37s - loss: 6.2106e-04 - acc: 0.04 - ETA: 34s - loss: 6.2105e-04 - acc: 0.04 - ETA: 30s - loss: 6.2112e-04 - acc: 0.04 - ETA: 27s - loss: 6.2120e-04 - acc: 0.04 - ETA: 24s - loss: 6.2127e-04 - acc: 0.04 - ETA: 21s - loss: 6.2118e-04 - acc: 0.04 - ETA: 18s - loss: 6.2123e-04 - acc: 0.04 - ETA: 15s - loss: 6.2117e-04 - acc: 0.04 - ETA: 12s - loss: 6.2108e-04 - acc: 0.04 - ETA: 9s - loss: 6.2096e-04 - acc: 0.0434 - ETA: 5s - loss: 6.2092e-04 - acc: 0.043 - ETA: 2s - loss: 6.2090e-04 - acc: 0.043 - 823s 14ms/step - loss: 6.2089e-04 - acc: 0.0435 - val_loss: 5.6125e-04 - val_acc: 3.8314e-05\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47616/60900 [======================>.......] - ETA: 12:32 - loss: 6.2468e-04 - acc: 0.02 - ETA: 12:22 - loss: 5.9797e-04 - acc: 0.04 - ETA: 12:13 - loss: 6.0055e-04 - acc: 0.04 - ETA: 12:16 - loss: 6.0549e-04 - acc: 0.04 - ETA: 12:21 - loss: 6.0771e-04 - acc: 0.04 - ETA: 12:16 - loss: 6.1067e-04 - acc: 0.04 - ETA: 12:00 - loss: 6.1318e-04 - acc: 0.04 - ETA: 11:55 - loss: 6.1594e-04 - acc: 0.03 - ETA: 11:51 - loss: 6.1385e-04 - acc: 0.04 - ETA: 11:47 - loss: 6.1200e-04 - acc: 0.04 - ETA: 11:38 - loss: 6.1312e-04 - acc: 0.04 - ETA: 11:33 - loss: 6.1273e-04 - acc: 0.04 - ETA: 11:31 - loss: 6.1401e-04 - acc: 0.04 - ETA: 11:28 - loss: 6.1805e-04 - acc: 0.04 - ETA: 11:21 - loss: 6.1844e-04 - acc: 0.04 - ETA: 11:21 - loss: 6.1760e-04 - acc: 0.04 - ETA: 11:20 - loss: 6.1762e-04 - acc: 0.04 - ETA: 11:16 - loss: 6.1868e-04 - acc: 0.04 - ETA: 11:14 - loss: 6.1851e-04 - acc: 0.04 - ETA: 11:10 - loss: 6.1874e-04 - acc: 0.04 - ETA: 11:07 - loss: 6.1815e-04 - acc: 0.04 - ETA: 11:05 - loss: 6.1797e-04 - acc: 0.04 - ETA: 11:01 - loss: 6.1725e-04 - acc: 0.04 - ETA: 10:56 - loss: 6.1710e-04 - acc: 0.04 - ETA: 10:52 - loss: 6.1767e-04 - acc: 0.04 - ETA: 10:50 - loss: 6.1725e-04 - acc: 0.04 - ETA: 10:46 - loss: 6.1658e-04 - acc: 0.04 - ETA: 10:41 - loss: 6.1578e-04 - acc: 0.04 - ETA: 10:40 - loss: 6.1604e-04 - acc: 0.04 - ETA: 10:37 - loss: 6.1602e-04 - acc: 0.04 - ETA: 10:30 - loss: 6.1639e-04 - acc: 0.04 - ETA: 10:22 - loss: 6.1653e-04 - acc: 0.04 - ETA: 10:16 - loss: 6.1681e-04 - acc: 0.04 - ETA: 10:11 - loss: 6.1647e-04 - acc: 0.04 - ETA: 10:05 - loss: 6.1623e-04 - acc: 0.04 - ETA: 10:00 - loss: 6.1588e-04 - acc: 0.04 - ETA: 9:55 - loss: 6.1566e-04 - acc: 0.0441 - ETA: 9:49 - loss: 6.1608e-04 - acc: 0.043 - ETA: 9:43 - loss: 6.1555e-04 - acc: 0.043 - ETA: 9:36 - loss: 6.1513e-04 - acc: 0.044 - ETA: 9:30 - loss: 6.1449e-04 - acc: 0.044 - ETA: 9:23 - loss: 6.1429e-04 - acc: 0.044 - ETA: 9:19 - loss: 6.1520e-04 - acc: 0.045 - ETA: 9:18 - loss: 6.1574e-04 - acc: 0.045 - ETA: 9:16 - loss: 6.1575e-04 - acc: 0.044 - ETA: 9:17 - loss: 6.1627e-04 - acc: 0.045 - ETA: 9:16 - loss: 6.1633e-04 - acc: 0.044 - ETA: 9:16 - loss: 6.1659e-04 - acc: 0.044 - ETA: 9:16 - loss: 6.1589e-04 - acc: 0.044 - ETA: 9:14 - loss: 6.1605e-04 - acc: 0.045 - ETA: 9:14 - loss: 6.1616e-04 - acc: 0.044 - ETA: 9:11 - loss: 6.1550e-04 - acc: 0.044 - ETA: 9:07 - loss: 6.1519e-04 - acc: 0.045 - ETA: 9:05 - loss: 6.1565e-04 - acc: 0.044 - ETA: 9:02 - loss: 6.1540e-04 - acc: 0.045 - ETA: 9:00 - loss: 6.1493e-04 - acc: 0.045 - ETA: 8:58 - loss: 6.1499e-04 - acc: 0.045 - ETA: 8:55 - loss: 6.1472e-04 - acc: 0.045 - ETA: 8:53 - loss: 6.1487e-04 - acc: 0.045 - ETA: 8:50 - loss: 6.1498e-04 - acc: 0.045 - ETA: 8:48 - loss: 6.1460e-04 - acc: 0.045 - ETA: 8:44 - loss: 6.1478e-04 - acc: 0.045 - ETA: 8:41 - loss: 6.1486e-04 - acc: 0.045 - ETA: 8:38 - loss: 6.1538e-04 - acc: 0.045 - ETA: 8:36 - loss: 6.1518e-04 - acc: 0.045 - ETA: 8:32 - loss: 6.1490e-04 - acc: 0.045 - ETA: 8:29 - loss: 6.1502e-04 - acc: 0.045 - ETA: 8:26 - loss: 6.1529e-04 - acc: 0.045 - ETA: 8:24 - loss: 6.1528e-04 - acc: 0.045 - ETA: 8:20 - loss: 6.1534e-04 - acc: 0.045 - ETA: 8:17 - loss: 6.1548e-04 - acc: 0.045 - ETA: 8:15 - loss: 6.1502e-04 - acc: 0.045 - ETA: 8:12 - loss: 6.1516e-04 - acc: 0.045 - ETA: 8:09 - loss: 6.1505e-04 - acc: 0.045 - ETA: 8:07 - loss: 6.1533e-04 - acc: 0.045 - ETA: 8:04 - loss: 6.1549e-04 - acc: 0.044 - ETA: 8:01 - loss: 6.1553e-04 - acc: 0.044 - ETA: 7:59 - loss: 6.1539e-04 - acc: 0.044 - ETA: 7:56 - loss: 6.1517e-04 - acc: 0.044 - ETA: 7:53 - loss: 6.1525e-04 - acc: 0.045 - ETA: 7:50 - loss: 6.1497e-04 - acc: 0.045 - ETA: 7:47 - loss: 6.1506e-04 - acc: 0.045 - ETA: 7:44 - loss: 6.1488e-04 - acc: 0.045 - ETA: 7:42 - loss: 6.1499e-04 - acc: 0.045 - ETA: 7:39 - loss: 6.1518e-04 - acc: 0.045 - ETA: 7:36 - loss: 6.1483e-04 - acc: 0.046 - ETA: 7:33 - loss: 6.1475e-04 - acc: 0.046 - ETA: 7:31 - loss: 6.1474e-04 - acc: 0.045 - ETA: 7:29 - loss: 6.1463e-04 - acc: 0.046 - ETA: 7:26 - loss: 6.1439e-04 - acc: 0.045 - ETA: 7:23 - loss: 6.1469e-04 - acc: 0.045 - ETA: 7:21 - loss: 6.1435e-04 - acc: 0.045 - ETA: 7:18 - loss: 6.1441e-04 - acc: 0.045 - ETA: 7:15 - loss: 6.1432e-04 - acc: 0.045 - ETA: 7:12 - loss: 6.1411e-04 - acc: 0.046 - ETA: 7:09 - loss: 6.1426e-04 - acc: 0.046 - ETA: 7:06 - loss: 6.1401e-04 - acc: 0.045 - ETA: 7:02 - loss: 6.1377e-04 - acc: 0.045 - ETA: 6:59 - loss: 6.1366e-04 - acc: 0.046 - ETA: 6:56 - loss: 6.1370e-04 - acc: 0.046 - ETA: 6:53 - loss: 6.1341e-04 - acc: 0.046 - ETA: 6:50 - loss: 6.1310e-04 - acc: 0.046 - ETA: 6:47 - loss: 6.1284e-04 - acc: 0.046 - ETA: 6:44 - loss: 6.1291e-04 - acc: 0.046 - ETA: 6:41 - loss: 6.1302e-04 - acc: 0.046 - ETA: 6:38 - loss: 6.1304e-04 - acc: 0.046 - ETA: 6:35 - loss: 6.1295e-04 - acc: 0.046 - ETA: 6:32 - loss: 6.1296e-04 - acc: 0.045 - ETA: 6:30 - loss: 6.1295e-04 - acc: 0.045 - ETA: 6:27 - loss: 6.1331e-04 - acc: 0.045 - ETA: 6:24 - loss: 6.1299e-04 - acc: 0.046 - ETA: 6:21 - loss: 6.1291e-04 - acc: 0.045 - ETA: 6:18 - loss: 6.1271e-04 - acc: 0.046 - ETA: 6:15 - loss: 6.1276e-04 - acc: 0.046 - ETA: 6:11 - loss: 6.1267e-04 - acc: 0.046 - ETA: 6:08 - loss: 6.1227e-04 - acc: 0.046 - ETA: 6:05 - loss: 6.1238e-04 - acc: 0.046 - ETA: 6:02 - loss: 6.1252e-04 - acc: 0.046 - ETA: 5:59 - loss: 6.1258e-04 - acc: 0.046 - ETA: 5:56 - loss: 6.1287e-04 - acc: 0.046 - ETA: 5:53 - loss: 6.1306e-04 - acc: 0.046 - ETA: 5:50 - loss: 6.1322e-04 - acc: 0.046 - ETA: 5:47 - loss: 6.1322e-04 - acc: 0.046 - ETA: 5:44 - loss: 6.1349e-04 - acc: 0.045 - ETA: 5:41 - loss: 6.1352e-04 - acc: 0.046 - ETA: 5:38 - loss: 6.1360e-04 - acc: 0.045 - ETA: 5:34 - loss: 6.1364e-04 - acc: 0.045 - ETA: 5:31 - loss: 6.1378e-04 - acc: 0.045 - ETA: 5:28 - loss: 6.1370e-04 - acc: 0.045 - ETA: 5:25 - loss: 6.1363e-04 - acc: 0.045 - ETA: 5:23 - loss: 6.1347e-04 - acc: 0.045 - ETA: 5:20 - loss: 6.1359e-04 - acc: 0.045 - ETA: 5:17 - loss: 6.1336e-04 - acc: 0.046 - ETA: 5:14 - loss: 6.1337e-04 - acc: 0.046 - ETA: 5:11 - loss: 6.1358e-04 - acc: 0.046 - ETA: 5:08 - loss: 6.1381e-04 - acc: 0.045 - ETA: 5:05 - loss: 6.1402e-04 - acc: 0.046 - ETA: 5:02 - loss: 6.1394e-04 - acc: 0.046 - ETA: 4:59 - loss: 6.1388e-04 - acc: 0.045 - ETA: 4:56 - loss: 6.1387e-04 - acc: 0.046 - ETA: 4:53 - loss: 6.1387e-04 - acc: 0.046 - ETA: 4:50 - loss: 6.1396e-04 - acc: 0.045 - ETA: 4:47 - loss: 6.1396e-04 - acc: 0.045 - ETA: 4:44 - loss: 6.1387e-04 - acc: 0.046 - ETA: 4:41 - loss: 6.1368e-04 - acc: 0.046 - ETA: 4:38 - loss: 6.1368e-04 - acc: 0.045 - ETA: 4:35 - loss: 6.1388e-04 - acc: 0.045 - ETA: 4:31 - loss: 6.1385e-04 - acc: 0.045 - ETA: 4:28 - loss: 6.1381e-04 - acc: 0.045 - ETA: 4:25 - loss: 6.1382e-04 - acc: 0.045 - ETA: 4:22 - loss: 6.1385e-04 - acc: 0.046 - ETA: 4:19 - loss: 6.1389e-04 - acc: 0.045 - ETA: 4:16 - loss: 6.1398e-04 - acc: 0.046 - ETA: 4:13 - loss: 6.1381e-04 - acc: 0.046 - ETA: 4:10 - loss: 6.1366e-04 - acc: 0.046 - ETA: 4:07 - loss: 6.1360e-04 - acc: 0.046 - ETA: 4:04 - loss: 6.1354e-04 - acc: 0.046 - ETA: 4:01 - loss: 6.1365e-04 - acc: 0.046 - ETA: 3:58 - loss: 6.1360e-04 - acc: 0.046 - ETA: 3:55 - loss: 6.1364e-04 - acc: 0.046 - ETA: 3:52 - loss: 6.1382e-04 - acc: 0.046 - ETA: 3:49 - loss: 6.1390e-04 - acc: 0.045 - ETA: 3:46 - loss: 6.1409e-04 - acc: 0.046 - ETA: 3:43 - loss: 6.1396e-04 - acc: 0.046 - ETA: 3:40 - loss: 6.1408e-04 - acc: 0.046 - ETA: 3:37 - loss: 6.1397e-04 - acc: 0.046 - ETA: 3:34 - loss: 6.1392e-04 - acc: 0.046 - ETA: 3:31 - loss: 6.1398e-04 - acc: 0.046 - ETA: 3:28 - loss: 6.1390e-04 - acc: 0.046 - ETA: 3:25 - loss: 6.1409e-04 - acc: 0.046 - ETA: 3:22 - loss: 6.1413e-04 - acc: 0.046 - ETA: 3:19 - loss: 6.1411e-04 - acc: 0.046 - ETA: 3:16 - loss: 6.1426e-04 - acc: 0.046 - ETA: 3:13 - loss: 6.1422e-04 - acc: 0.046 - ETA: 3:10 - loss: 6.1421e-04 - acc: 0.046 - ETA: 3:07 - loss: 6.1420e-04 - acc: 0.046 - ETA: 3:04 - loss: 6.1445e-04 - acc: 0.046 - ETA: 3:01 - loss: 6.1437e-04 - acc: 0.046 - ETA: 2:58 - loss: 6.1458e-04 - acc: 0.046 - ETA: 2:55 - loss: 6.1460e-04 - acc: 0.046 - ETA: 2:52 - loss: 6.1451e-04 - acc: 0.046 - ETA: 2:49 - loss: 6.1454e-04 - acc: 0.046 - ETA: 2:46 - loss: 6.1438e-04 - acc: 0.046 - ETA: 2:43 - loss: 6.1421e-04 - acc: 0.046 - ETA: 2:40 - loss: 6.1422e-04 - acc: 0.046 - ETA: 2:37 - loss: 6.1415e-04 - acc: 0.046760900/60900 [==============================] - ETA: 2:34 - loss: 6.1397e-04 - acc: 0.046 - ETA: 2:31 - loss: 6.1395e-04 - acc: 0.046 - ETA: 2:28 - loss: 6.1379e-04 - acc: 0.047 - ETA: 2:25 - loss: 6.1368e-04 - acc: 0.047 - ETA: 2:22 - loss: 6.1360e-04 - acc: 0.047 - ETA: 2:19 - loss: 6.1364e-04 - acc: 0.047 - ETA: 2:16 - loss: 6.1365e-04 - acc: 0.047 - ETA: 2:13 - loss: 6.1365e-04 - acc: 0.047 - ETA: 2:10 - loss: 6.1369e-04 - acc: 0.047 - ETA: 2:07 - loss: 6.1394e-04 - acc: 0.047 - ETA: 2:04 - loss: 6.1380e-04 - acc: 0.047 - ETA: 2:01 - loss: 6.1373e-04 - acc: 0.047 - ETA: 1:58 - loss: 6.1371e-04 - acc: 0.047 - ETA: 1:54 - loss: 6.1356e-04 - acc: 0.047 - ETA: 1:51 - loss: 6.1351e-04 - acc: 0.047 - ETA: 1:48 - loss: 6.1348e-04 - acc: 0.047 - ETA: 1:45 - loss: 6.1339e-04 - acc: 0.047 - ETA: 1:42 - loss: 6.1335e-04 - acc: 0.047 - ETA: 1:39 - loss: 6.1335e-04 - acc: 0.047 - ETA: 1:36 - loss: 6.1335e-04 - acc: 0.047 - ETA: 1:33 - loss: 6.1333e-04 - acc: 0.047 - ETA: 1:30 - loss: 6.1331e-04 - acc: 0.047 - ETA: 1:27 - loss: 6.1326e-04 - acc: 0.047 - ETA: 1:24 - loss: 6.1333e-04 - acc: 0.047 - ETA: 1:21 - loss: 6.1331e-04 - acc: 0.047 - ETA: 1:18 - loss: 6.1322e-04 - acc: 0.047 - ETA: 1:15 - loss: 6.1314e-04 - acc: 0.048 - ETA: 1:12 - loss: 6.1314e-04 - acc: 0.048 - ETA: 1:09 - loss: 6.1325e-04 - acc: 0.048 - ETA: 1:06 - loss: 6.1329e-04 - acc: 0.047 - ETA: 1:03 - loss: 6.1336e-04 - acc: 0.047 - ETA: 1:00 - loss: 6.1330e-04 - acc: 0.048 - ETA: 57s - loss: 6.1323e-04 - acc: 0.047 - ETA: 54s - loss: 6.1327e-04 - acc: 0.04 - ETA: 51s - loss: 6.1329e-04 - acc: 0.04 - ETA: 48s - loss: 6.1340e-04 - acc: 0.04 - ETA: 45s - loss: 6.1342e-04 - acc: 0.04 - ETA: 42s - loss: 6.1330e-04 - acc: 0.04 - ETA: 39s - loss: 6.1324e-04 - acc: 0.04 - ETA: 36s - loss: 6.1330e-04 - acc: 0.04 - ETA: 33s - loss: 6.1336e-04 - acc: 0.04 - ETA: 30s - loss: 6.1343e-04 - acc: 0.04 - ETA: 27s - loss: 6.1349e-04 - acc: 0.04 - ETA: 24s - loss: 6.1348e-04 - acc: 0.04 - ETA: 20s - loss: 6.1353e-04 - acc: 0.04 - ETA: 17s - loss: 6.1343e-04 - acc: 0.04 - ETA: 14s - loss: 6.1328e-04 - acc: 0.04 - ETA: 11s - loss: 6.1334e-04 - acc: 0.04 - ETA: 8s - loss: 6.1327e-04 - acc: 0.0479 - ETA: 5s - loss: 6.1332e-04 - acc: 0.048 - ETA: 2s - loss: 6.1323e-04 - acc: 0.048 - 793s 13ms/step - loss: 6.1324e-04 - acc: 0.0482 - val_loss: 5.6137e-04 - val_acc: 7.6628e-05\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-9f01e32b1afa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tags_model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense,Embedding,Conv1D,Dropout,LSTM,MaxPool1D\n",
    "from keras.models import Sequential\n",
    "\n",
    "embedding_layer = Embedding(len(tokenizer.word_index),300,weights = [embedding_matrix],trainable=False)\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Conv1D(filters=512,kernel_size=7,activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=5))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Conv1D(filters=512,kernel_size=7,activation='relu'))\n",
    "model.add(MaxPool1D(pool_size=5))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(300,activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(len(one_hot.classes_), activation='relu'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(train_X,train_y,validation_data=(val_X,val_y),epochs=3,batch_size=256,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('tags_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('tags_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 300)         48883500  \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, None, 512)         1075712   \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, None, 512)         1835520   \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, None, 512)         0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               245200    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               30300     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4268)              1284668   \n",
      "=================================================================\n",
      "Total params: 53,354,900\n",
      "Trainable params: 4,471,400\n",
      "Non-trainable params: 48,883,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('transfer-learning-on-stack-exchange-tags/test.csv')\n",
    "test_data['content'] = test_data['content'].apply(lambda x:remove_html_tags(x))\n",
    "test_data['content'] = test_data['content'].apply(lambda x:remove_punctuation(x))\n",
    "test_data['content'] = test_data['content'].apply(lambda x:remove_stopwords(x))\n",
    "test_data['content'] = test_data['content'].apply(lambda x:lemmatize_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pad_sequences(tokenizer.texts_to_sequences(list(test_data['content'])),maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81926/81926 [==============================] - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 59s - ETA: 58 - ETA: 58 - ETA: 57 - ETA: 57 - ETA: 56 - ETA: 56 - ETA: 55 - ETA: 54 - ETA: 54 - ETA: 53 - ETA: 53 - ETA: 52 - ETA: 52 - ETA: 51 - ETA: 50 - ETA: 50 - ETA: 49 - ETA: 49 - ETA: 48 - ETA: 48 - ETA: 47 - ETA: 47 - ETA: 46 - ETA: 46 - ETA: 45 - ETA: 44 - ETA: 44 - ETA: 43 - ETA: 43 - ETA: 42 - ETA: 42 - ETA: 41 - ETA: 40 - ETA: 40 - ETA: 39 - ETA: 39 - ETA: 38 - ETA: 38 - ETA: 37 - ETA: 36 - ETA: 36 - ETA: 35 - ETA: 35 - ETA: 34 - ETA: 34 - ETA: 33 - ETA: 32 - ETA: 32 - ETA: 31 - ETA: 31 - ETA: 30 - ETA: 30 - ETA: 29 - ETA: 29 - ETA: 28 - ETA: 27 - ETA: 27 - ETA: 26 - ETA: 26 - ETA: 25 - ETA: 25 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 182s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(test_X,batch_size=256,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   28.7s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=4)]: Done 1242 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=4)]: Done 1792 tasks      | elapsed: 18.7min\n",
      "[Parallel(n_jobs=4)]: Done 2442 tasks      | elapsed: 26.7min\n",
      "[Parallel(n_jobs=4)]: Done 3192 tasks      | elapsed: 35.3min\n",
      "[Parallel(n_jobs=4)]: Done 4042 tasks      | elapsed: 44.7min\n",
      "[Parallel(n_jobs=4)]: Done 4268 out of 4268 | elapsed: 47.1min finished\n",
      "c:\\users\\kushal\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\joblib\\disk.py:122: UserWarning: Unable to delete folder C:\\Users\\kushal\\AppData\\Local\\Temp\\joblib_memmapping_folder_16796_4664122654 after 5 tentatives.\n",
      "  .format(folder_path, RM_SUBDIRS_N_RETRY))\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\kushal\\\\AppData\\\\Local\\\\Temp\\\\joblib_memmapping_folder_16796_4664122654\\\\16796-3159832263704-7040d39116f94e3c8e7ba92a31a536eb.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-eefd768562cb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0macc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mbest_threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbestThreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\kushal\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    942\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    943\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 944\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_terminate_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    945\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pickle_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kushal\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_terminate_backend\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_terminate_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kushal\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mterminate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;31m# in latter calls but we free as much memory as we can by deleting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[1;31m# the shared memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 530\u001b[1;33m             \u001b[0mdelete_folder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_temp_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kushal\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\joblib\\disk.py\u001b[0m in \u001b[0;36mdelete_folder\u001b[1;34m(folder_path, onerror)\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m                     \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWindowsError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kushal\\appdata\\local\\programs\\python\\python36\\lib\\shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_rmtree_unsafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;31m# Allow introspection of whether or not the hardening against symlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kushal\\appdata\\local\\programs\\python\\python36\\lib\\shutil.py\u001b[0m in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    387\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m                 \u001b[0monerror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\kushal\\appdata\\local\\programs\\python\\python36\\lib\\shutil.py\u001b[0m in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 387\u001b[1;33m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    388\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m                 \u001b[0monerror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfullname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\kushal\\\\AppData\\\\Local\\\\Temp\\\\joblib_memmapping_folder_16796_4664122654\\\\16796-3159832263704-7040d39116f94e3c8e7ba92a31a536eb.pkl'"
     ]
    }
   ],
   "source": [
    "from joblib import Parallel,delayed\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "threshold = np.arange(0,0.02,0.00025)\n",
    "out = model.predict(val_X)\n",
    "out = np.array(out)\n",
    "def bestThreshold(y_prob, threshold, i):\n",
    "    acc = []\n",
    "    for j in threshold:\n",
    "        y_pred = np.greater_equal(y_prob, j)*1\n",
    "        acc.append(matthews_corrcoef(val_y[:,i], y_pred))\n",
    "    acc = np.array(acc)\n",
    "    index = np.where(acc==acc.max())\n",
    "    return threshold[index[0][0]]\n",
    "best_threshold = Parallel(n_jobs=4, verbose=1)(delayed(bestThreshold)(out[:,i], threshold, i) for i in range(out.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_threshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-dccc364758bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_threshold\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'best_threshold' is not defined"
     ]
    }
   ],
   "source": [
    "best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
